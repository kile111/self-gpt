import os
import json
import tempfile
from pathlib import Path
from dotenv import load_dotenv
import streamlit as st
from openai import OpenAI
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import av
import requests
import easyocr
from utils.file_reader import read_file

# ===== åˆå§‹åŒ– =====
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
serpapi_key = os.getenv("SERP_API_KEY")
client = OpenAI(api_key=api_key)
MEMORY_FILE = "chat_memory.json"

# ===== èŠå¤©è®°å¿† =====
def load_memory():
    if Path(MEMORY_FILE).exists():
        return json.loads(Path(MEMORY_FILE).read_text(encoding="utf-8"))
    return []
def save_memory(history):
    Path(MEMORY_FILE).write_text(json.dumps(history, ensure_ascii=False, indent=2), encoding="utf-8")
if "chat_history" not in st.session_state:
    st.session_state.chat_history = load_memory()

# ===== è‡ªåŠ¨è§¦å‘è”ç½‘æœç´¢åˆ¤æ–­ =====
def need_live_search(user_prompt: str):
    keywords = [
        # é‡‘è/å¸ä»·
        "ä»·æ ¼", "æœ€æ–°ä»·æ ¼", "å½“å‰ä»·æ ¼", "å®æ—¶ä»·æ ¼",
        "BTC", "æ¯”ç‰¹å¸", "ETH", "ä»¥å¤ªåŠ",
        "è‚¡ç¥¨", "è‚¡ä»·", "ä¸Šæ¶¨", "ä¸‹è·Œ",
        "ç¾å…ƒæ±‡ç‡", "äººæ°‘å¸æ±‡ç‡", "æ±‡ç‡",
        # æ–°é—»
        "æ–°é—»", "æœ€æ–°æ¶ˆæ¯", "å®æ—¶æ–°é—»", "å‘ç”Ÿäº†ä»€ä¹ˆ", "åˆšåˆšå‘ç”Ÿ",
        # æ—¶é—´
        "ä»Šå¤©", "ç°åœ¨", "æœ€æ–°", "æœ€è¿‘", "å½“å‰",
        # å¤©æ°”
        "å¤©æ°”", "æ°”æ¸©", "æ°”å€™", "é™é›¨", "å°é£", "å®æ—¶å¤©æ°”",
        # ä½“è‚²
        "æ¯”èµ›ç»“æœ", "èµ›æœ", "å®æ—¶æ¯”åˆ†", "è¿›çƒ", "æ¯”åˆ†", "å† å†›"
    ]
    for kw in keywords:
        if kw.lower() in user_prompt.lower():
            return True
    return False

# ===== OCR å›¾ç‰‡è§£æ =====
def ocr_image_easyocr(uploaded_file):
    temp_dir = tempfile.mkdtemp()
    temp_path = os.path.join(temp_dir, uploaded_file.name)
    with open(temp_path, "wb") as f:
        f.write(uploaded_file.read())
    reader = easyocr.Reader(['ch', 'en'])
    result = reader.readtext(temp_path)
    return "\n".join([res[1] for res in result])

# ===== è”ç½‘æœç´¢ =====
def search_web_serpapi(query, num_results=5):
    url = "https://serpapi.com/search.json"
    params = {"q": query, "hl": "zh-cn", "gl": "cn", "num": num_results, "api_key": serpapi_key}
    resp = requests.get(url, params=params)
    data = resp.json()
    results_text = []
    if "organic_results" in data:
        for item in data["organic_results"]:
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            link = item.get("link", "")
            results_text.append(f"{title}\n{snippet}\n{link}\n")
    return "\n".join(results_text) if results_text else "æ²¡æœ‰æœç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

# ===== Whisper è¯­éŸ³è¯†åˆ« =====
class SpeechToTextProcessor(AudioProcessorBase):
    def __init__(self):
        self.frames = []
    def recv_audio_frame(self, frame: av.AudioFrame) -> av.AudioFrame:
        self.frames.append(frame)
        return frame

def audio_frames_to_wav(frames, filename="temp.wav"):
    import wave
    import numpy as np
    if not frames: return None
    audio = np.concatenate([f.to_ndarray() for f in frames])
    with wave.open(filename, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(16000)
        wf.writeframes(audio.tobytes())
    return filename

def speech_to_text(frames):
    wav_file = audio_frames_to_wav(frames)
    if not wav_file: return ""
    with open(wav_file, "rb") as f:
        transcript = client.audio.transcriptions.create(model="whisper-1", file=f)
    return transcript.text

# ===== GPT-5 æ¨ç†ï¼ˆå¯éšè—æ¨ç†è¿‡ç¨‹ï¼‰ =====
def gpt_ask(user_input, system_prompt=None, effort="high", reasoning_tokens=1200, output_tokens=800):
    if system_prompt is None:
        system_prompt = st.session_state.get(
            "system_prompt",
            "ä½ æ˜¯ä¸€ä¸ªå…·å¤‡é«˜çº§æ¨ç†èƒ½åŠ›çš„ GPTâ€‘5 æ¨¡å‹ï¼Œè¯·åœ¨å›ç­”ä¸­åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šå…ˆå†™æ¨ç†è¿‡ç¨‹ï¼Œå†å†™æœ€ç»ˆç­”æ¡ˆã€‚"
        )

    effort_map = {
        "low": "ç®€çŸ­æ¦‚è¿°æ¨ç†è¿‡ç¨‹å¹¶å¿«é€Ÿç»™å‡ºç»“è®ºã€‚",
        "medium": "åˆ†æ­¥éª¤æ¨ç†å¹¶æœ‰æ¡ç†åœ°å¾—å‡ºç»“è®ºã€‚",
        "high": "è¯¦ç»†å±•ç¤ºæ¨ç†é“¾è·¯ï¼ˆChain-of-Thoughtï¼‰ï¼Œåˆ†ææ¯ä¸ªå…³é”®å› ç´ ï¼Œæœ€åç»™å‡ºæ˜ç¡®ç»“è®ºã€‚"
    }

    reasoning_prompt = f"""
{effort_map.get(effort, effort_map['high'])}
è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼š
æ¨ç†è¿‡ç¨‹:
...(æ¨ç†éƒ¨åˆ†)...
æœ€ç»ˆç­”æ¡ˆ:
...(ç®€æ´ç»“è®º)...
é™åˆ¶æ¨ç†tokensä¸è¶…è¿‡ {reasoning_tokens}ã€‚
é—®é¢˜ï¼š{user_input}
    """

    messages = [
        {"role": "system", "content": system_prompt},
        *st.session_state.chat_history,
        {"role": "user", "content": reasoning_prompt}
    ]
    
    reply_obj = client.chat.completions.create(
        model="gpt-5-chat-latest",
        messages=messages,
        max_tokens=output_tokens
    )

    full_answer = reply_obj.choices[0].message.content

    if not st.session_state.show_reasoning:
        if "æœ€ç»ˆç­”æ¡ˆ:" in full_answer:
            answer = full_answer.split("æœ€ç»ˆç­”æ¡ˆ:")[-1].strip()
        else:
            answer = full_answer
    else:
        answer = full_answer

    st.session_state.chat_history.append({"role": "user", "content": user_input})
    st.session_state.chat_history.append({"role": "assistant", "content": answer})
    save_memory(st.session_state.chat_history)
    return answer

# ===== æ–‡æœ¬è½¬è¯­éŸ³ =====
def speak_text(text):
    audio_file = "output.mp3"
    with client.audio.speech.with_streaming_response.create(model="gpt-4o-mini-tts", voice="alloy", input=text) as response:
        response.stream_to_file(audio_file)
    st.audio(audio_file, format="audio/mp3")

# ===== é¡µé¢ UI =====
st.set_page_config(page_title="GPT-5 è¶…çº§åŠ©æ‰‹", page_icon="ğŸ¦„", layout="wide")
st.title("ğŸ¦„ GPTâ€‘5 è¶…çº§æ•´åˆåŠ©æ‰‹ï¼ˆç»ˆæç‰ˆ)")

# Sidebar
mode = st.sidebar.radio("é€‰æ‹©æ¨¡å¼", ["èŠå¤©", "åŒå£°ä¼ è¯‘", "å®æ—¶å­—å¹•åŒä¼ "])
default_prompt = "ä½ æ˜¯åŸºäº GPTâ€‘5 æä¾›å‡†ç¡®ã€æœ‰æ¡ç†å›ç­”çš„æ¨¡å‹ã€‚"
st.session_state.system_prompt = st.sidebar.text_area("ğŸ“ è‡ªå®šä¹‰è§’è‰²è®¾å®š", value=st.session_state.get("system_prompt", default_prompt), height=100)
effort = st.sidebar.selectbox("æ¨ç†ç­‰çº§", ["low", "medium", "high"], index=2)
reasoning_tokens = st.sidebar.slider("æ¨ç† tokens", 200, 2000, 1200, step=100)
output_tokens = st.sidebar.slider("è¾“å‡º tokens", 200, 2000, 800, step=100)
st.session_state.show_reasoning = st.sidebar.checkbox("æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹", value=True)
if st.sidebar.button("ğŸ—‘ æ¸…ç©ºè®°å¿†"):
    st.session_state.chat_history = []
    save_memory([])
    st.rerun()

# ===== èŠå¤©æ¨¡å¼ =====
if mode == "èŠå¤©":
    for chat in st.session_state.chat_history:
        with st.chat_message(chat["role"]):
            st.markdown(chat["content"])

    # è¯­éŸ³è¾“å…¥
    rtc_ctx = webrtc_streamer(key="chat-voice", mode=WebRtcMode.SENDONLY,
                              audio_processor_factory=SpeechToTextProcessor,
                              media_stream_constraints={"audio": True, "video": False})
    if rtc_ctx.audio_processor and rtc_ctx.audio_processor.frames:
        if st.button("ğŸ¤ è¯­éŸ³è½¬æ–‡å­—"):
            st.session_state.voice_input = speech_to_text(rtc_ctx.audio_processor.frames)

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_files = st.file_uploader(
        "ğŸ“ ä¸Šä¼ æ–‡ä»¶æˆ–å›¾ç‰‡ (PDF/DOCX/TXT/CSV/Excel/JPG/PNG)",
        type=["pdf", "docx", "txt", "csv", "xlsx", "png", "jpg", "jpeg"],
        accept_multiple_files=True
    )

    file_contents = ""
    if uploaded_files:
        for file in uploaded_files:
            suffix = Path(file.name).suffix.lower()
            if suffix in [".png", ".jpg", ".jpeg"]:
                file_contents += f"\n[å›¾ç‰‡ {file.name} OCR]:\n{ocr_image_easyocr(file)}\n"
            else:
                file_contents += f"\n[æ–‡ä»¶ {file.name} å†…å®¹]:\n{read_file(file)}\n"

    # ä¸»è¾“å…¥
    if st.session_state.get("voice_input"):
        user_prompt = st.session_state.pop("voice_input")
    else:
        user_prompt = st.chat_input("è¯·è¾“å…¥...")

    if user_prompt or file_contents:
        merged_prompt = ""
        if file_contents:
            merged_prompt += f"ä»¥ä¸‹æ˜¯ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶/å›¾ç‰‡è§£æç»“æœï¼š\n{file_contents}\n\n"

        # è‡ªåŠ¨è§¦å‘è”ç½‘
        if user_prompt and need_live_search(user_prompt):
            search_results = search_web_serpapi(user_prompt)
            merged_prompt += f"ä»¥ä¸‹æ˜¯å®æ—¶è”ç½‘æœç´¢çš„ç»“æœï¼š\n{search_results}\n\nè¯·ç»“åˆä»¥ä¸Šèµ„æ–™å›ç­”é—®é¢˜ï¼š{user_prompt}"

        # å›¾ç‰‡ç”Ÿæˆ
        elif user_prompt and ("ç”»" in user_prompt or "ç”Ÿæˆå›¾ç‰‡" in user_prompt.lower()):
            with st.spinner("ğŸ¨ AI æ­£åœ¨ä¸ºä½ ç”»å›¾..."):
                image_resp = client.images.generate(model="gpt-image-1", prompt=user_prompt, size="1024x1024")
                st.image(image_resp.data[0].url, caption="AIç”Ÿæˆå›¾ç‰‡")
            merged_prompt = None
        else:
            merged_prompt += user_prompt if user_prompt else ""

        if merged_prompt:
            reply = gpt_ask(
                merged_prompt.strip(),
                system_prompt=st.session_state.system_prompt,
                effort=effort,
                reasoning_tokens=reasoning_tokens,
                output_tokens=output_tokens
            )
            st.markdown(reply)
            if st.button("ğŸ”Š æ’­æ”¾å›ç­”"):
                speak_text(reply)

# ===== å…¶ä»–æ¨¡å¼ =====
elif mode == "åŒå£°ä¼ è¯‘":
    st.write("ğŸ§ åŒå£°ä¼ è¯‘æ¨¡å¼...ï¼ˆä¿ç•™å®ç°ï¼‰")
elif mode == "å®æ—¶å­—å¹•åŒä¼ ":
    st.write("ğŸ“º å®æ—¶å­—å¹•åŒä¼ æ¨¡å¼...ï¼ˆä¿ç•™å®ç°ï¼‰")